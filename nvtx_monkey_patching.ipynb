{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.nvtx as nvtx\n",
    "import inspect\n",
    "from inspect import currentframe, getargvalues, getfullargspec, getmembers, isfunction\n",
    "import types\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built-ins (manual monkey-patching required):\n",
      "['adaptive_avg_pool1d', 'avg_pool1d', 'avg_pool2d', 'avg_pool3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'elu_', 'hardtanh_', 'leaky_relu_', 'logsigmoid', 'prelu', 'relu_', 'rrelu_', 'selu_', 'softplus', 'softshrink', 'threshold_']\n",
      "Functions registered for NVTX range annotation:\n",
      "{'torch.nn.functional.instance_norm', 'torch.nn.functional.hardtanh', 'torch.nn.functional.max_pool3d', 'torch.nn.functional.grid_sample', 'torch.nn.functional.adaptive_avg_pool3d', 'torch.nn.functional.binary_cross_entropy', 'torch.nn.functional.cosine_similarity', 'torch.nn.functional.smooth_l1_loss', 'torch.nn.functional.gumbel_softmax', 'torch.nn.functional.nll_loss', 'torch.nn.functional.softmax', 'torch.nn.functional.adaptive_avg_pool2d', 'torch.nn.functional.lp_pool2d', 'torch.nn.functional.adaptive_max_pool3d', 'torch.nn.functional.fold', 'torch.nn.functional.normalize', 'torch.nn.functional.pad', 'torch.nn.functional.max_unpool3d', 'torch.nn.functional.selu', 'torch.nn.functional.upsample', 'torch.nn.functional.lp_pool1d', 'torch.nn.functional.adaptive_max_pool2d', 'torch.nn.functional.interpolate', 'torch.nn.functional.local_response_norm', 'torch.nn.functional.upsample_bilinear', 'torch.nn.functional.dropout3d', 'torch.nn.functional.max_unpool2d', 'torch.nn.functional.pairwise_distance', 'torch.nn.functional.cross_entropy', 'torch.nn.functional.leaky_relu', 'torch.nn.functional.layer_norm', 'torch.nn.functional.hardshrink', 'torch.nn.functional.embedding', 'torch.nn.functional.linear', 'torch.nn.functional.max_pool1d', 'torch.nn.functional.bilinear', 'torch.nn.functional.max_unpool1d', 'torch.nn.functional.relu', 'torch.nn.functional.threshold', 'torch.nn.functional.alpha_dropout', 'torch.nn.functional.poisson_nll_loss', 'torch.nn.functional.binary_cross_entropy_with_logits', 'torch.nn.functional.relu6', 'torch.nn.functional.soft_margin_loss', 'torch.nn.functional.tanhshrink', 'torch.nn.functional.dropout2d', 'torch.nn.functional.unfold', 'torch.nn.functional.softmin', 'torch.nn.functional.max_pool2d', 'torch.nn.functional.rrelu', 'torch.nn.functional.glu', 'torch.nn.functional.cosine_embedding_loss', 'torch.nn.functional.dropout', 'torch.nn.functional.batch_norm', 'torch.nn.functional.kl_div', 'torch.nn.functional.triplet_margin_loss', 'torch.nn.functional.l1_loss', 'torch.nn.functional.upsample_nearest', 'torch.nn.functional.affine_grid', 'torch.nn.functional.hinge_embedding_loss', 'torch.nn.functional.embedding_bag', 'torch.nn.functional.pixel_shuffle', 'torch.nn.functional.mse_loss', 'torch.nn.functional.tanh', 'torch.nn.functional.elu', 'torch.nn.functional.adaptive_max_pool1d', 'torch.nn.functional.log_softmax', 'torch.nn.functional.sigmoid', 'torch.nn.functional.softsign'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NvtxPatcher:\n",
    "    \n",
    "    registry = set()\n",
    "    nvtx_handle = nvtx._libnvToolsExt()\n",
    "    \n",
    "    @staticmethod\n",
    "    def nvtx_monkey_patch(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            frame = currentframe()\n",
    "            v = getargvalues(frame)\n",
    "            argspec = getfullargspec(func)\n",
    "            formal_arg_names = argspec.args\n",
    "            s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "            for idx, val in enumerate(v.locals[\"args\"]):\n",
    "                name = formal_arg_names[idx]\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    name += \"_shape\"\n",
    "                    val = tuple(val.size())\n",
    "                s += \"'%s':%s,\" % (name, str(val))\n",
    "            num_def=len(argspec.defaults)\n",
    "            defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "            overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "            defaults.update(overrides)\n",
    "            s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "            nvtx.range_push(s)\n",
    "            result = func(*args, **kwargs)\n",
    "            nvtx.range_pop()\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    @classmethod\n",
    "    def list_non_builtins(cls, module, regex_filt_lst=None, log=True):\n",
    "        if not isinstance(regex_filt_lst, list) and regex_filt_lst is not None:\n",
    "            regex_filt_lst = list(regex_filt_lst)\n",
    "        if isinstance(module, str):\n",
    "            module = eval(module)\n",
    "        name_list = dir(module)\n",
    "        builtin_funcs_methods = [_a for _a in name_list if\n",
    "                                 (isinstance(getattr(module, _a), types.BuiltinFunctionType) or\n",
    "                                  isinstance(getattr(module, _a), types.BuiltinMethodType))]\n",
    "        match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "        if regex_filt_lst is not None:\n",
    "            function_list = [_x for _x in builtin_funcs_methods if match_any(_x)]\n",
    "        else: \n",
    "            function_list = [_x for _x in builtin_funcs_methods]\n",
    "        return function_list \n",
    "                                 \n",
    "    @classmethod\n",
    "    def register_non_builtins(cls, module, regex_filt_lst=None, log=True):\n",
    "        if not isinstance(regex_filt_lst, list) and regex_filt_lst is not None:\n",
    "            regex_filt_lst = list(regex_filt_lst)\n",
    "        if isinstance(module, str):\n",
    "            module = eval(module)\n",
    "        name_list = dir(module)\n",
    "        non_builtin_funcs = [_a for _a in name_list if\n",
    "                     isinstance(getattr(module, _a), types.FunctionType)]\n",
    "        \n",
    "        match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "        if regex_filt_lst is not None:\n",
    "            function_list = [_x for _x in non_builtin_funcs if match_any(_x)]\n",
    "        else: \n",
    "            function_list = [_x for _x in mod_funcs]\n",
    "            \n",
    "        for name in function_list:\n",
    "            if name in cls.registry:\n",
    "                continue\n",
    "            fqn = \"{}.{}\".format(module.__name__, name)\n",
    "            temp = eval(fqn)\n",
    "            patched = NvtxPatcher.nvtx_monkey_patch(temp)\n",
    "            cls.registry.add(fqn)\n",
    "            exec(\"{}=patched\".format(fqn))\n",
    "            \n",
    "    @classmethod        \n",
    "    # convNd is a built-in, so can't be registered using the non-builtin approach above\n",
    "    def patch_conv(cls, dim_count, module=torch.nn.functional):\n",
    "        fun_name = \"{}.conv{}d\".format(module.__name__, str(dim_count))\n",
    "        # Function already patched\n",
    "        if fun_name in cls.registry:\n",
    "            return\n",
    "        temp = eval(fun_name)\n",
    "        def decorator(fun):\n",
    "            def wrapper(data, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "                print(\"### {} ###\".format(data))\n",
    "                input_size = tuple(data.size())\n",
    "                weight_size = tuple(weight.size())\n",
    "                # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "                nvtx_str = \"{op:'conv%sd', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % (dim_count,input_size, weight_size, str(stride), str(padding), str(dilation), str(groups))\n",
    "                nvtx.range_push(nvtx_str)\n",
    "                op = fun(data, weight, bias, stride, padding, dilation, groups)\n",
    "                nvtx.range_pop()\n",
    "                return op\n",
    "            return wrapper\n",
    "        patched = decorator(temp)\n",
    "        exec(\"{}=patched\".format(fun_name))\n",
    "        return patched\n",
    "              \n",
    "    @classmethod\n",
    "    def print_registered_functions(cls):\n",
    "              print(\"Functions registered for NVTX range annotation:\\n{}\\n\".format(str(cls.registry)))\n",
    "\n",
    "    \n",
    "patterns = [\"conv[1-3]?(d|(\\_transpose[1-3]d))\",\n",
    "     \"(un)?fold\",\n",
    "     \"(avg|max)_pool\",\n",
    "     \"max_unpool[1-3]d\",\n",
    "     \"lp_pool[1-3]d\",\n",
    "     \"adaptive_(avg|max)_pool[1-3]d\",\n",
    "     \"threshold\",\n",
    "     \"(leaky_)?[p-s]?r?elu_?6?\",\n",
    "     \"(hard)?tanh\",\n",
    "     \"glu\",\n",
    "     \"(log)?sigmoid\",\n",
    "     \"(hard|soft|tanh)shrink\",\n",
    "     \"soft(sign|plus|min)\",\n",
    "     \"(gumbel_|log_)?softmax\",\n",
    "     \"(batch|layer|instance|local_response)_norm\",\n",
    "     \"normalize\",\n",
    "     \"(bi)?linear\",\n",
    "     \"(alpha_)?dropout([2-3]d)?\",\n",
    "     \"embedding(_bag)?\",\n",
    "     \"pairwise_distance\",\n",
    "     \"cosine_similarity\",\n",
    "     \"(binary_)?cross_entropy\",\n",
    "     \"(poisson_)?nll_loss\",\n",
    "     \"(cosine|hinge)_embedding_loss\",\n",
    "     \"kl_div\",\n",
    "     \"((smooth_)?l1|mse)_loss\",\n",
    "     \"(multilabel|multi)?_margin_(soft_?)(ranking)?_loss\",\n",
    "     \"(soft|triplet)_margin_loss\",\n",
    "     \"pad\",\n",
    "     \"pixel_shuffle\",\n",
    "     \"interpolate\",\n",
    "     \"upsample_?(bilinear|nearest)?\",\n",
    "     \"(affine_)?grid(_sample)?\"]\n",
    "\n",
    "NvtxPatcher.register_non_builtins(\n",
    "    torch.nn.functional, patterns)\n",
    "                    \n",
    "for i in range(1, 4):\n",
    "    NvtxPatcher.patch_conv(i)               \n",
    "\n",
    "print(\"built-ins (manual monkey-patching required):\")\n",
    "print(NvtxPatcher.list_non_builtins(torch.nn.functional, patterns))\n",
    "                    \n",
    "NvtxPatcher.print_registered_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tensor([[[[ 0.6323,  0.5867, -0.6936,  0.6546, -0.8506],\n",
      "          [ 0.2374, -0.0905,  0.6267,  1.8584, -0.3787],\n",
      "          [ 0.1963,  1.3841,  1.2110,  1.0826,  0.5698],\n",
      "          [ 0.0248, -1.3243,  2.0463, -0.3436, -0.0651],\n",
      "          [-1.0432,  0.2888,  1.8106,  0.4096,  0.3805]],\n",
      "\n",
      "         [[ 1.8079, -2.4335,  2.3370,  0.1876, -1.8761],\n",
      "          [ 0.2383, -0.5957, -0.2922, -1.1921,  0.3144],\n",
      "          [ 0.0701, -0.9783,  0.8120,  0.9708, -0.8644],\n",
      "          [ 0.8355, -0.9670,  1.5134, -0.7500,  0.8595],\n",
      "          [-1.2391,  1.6980,  0.3484, -3.1029, -0.8725]],\n",
      "\n",
      "         [[-0.9943,  0.5533,  0.0658, -0.6878, -2.0404],\n",
      "          [ 0.4509, -0.2956,  0.8767, -0.8130,  0.4259],\n",
      "          [-1.6613,  1.4032, -0.6154, -0.6392, -0.3006],\n",
      "          [-1.9183, -0.8526,  0.4259, -0.5146, -0.2385],\n",
      "          [ 0.2143,  1.3863,  0.6244,  1.2601,  2.0780]]],\n",
      "\n",
      "\n",
      "        [[[-0.1714, -0.4178,  3.2297,  1.2704, -0.5574],\n",
      "          [ 0.2940,  0.1871,  1.2556, -0.4744,  0.8085],\n",
      "          [ 0.1565,  0.1315, -0.1820, -0.6173, -1.8289],\n",
      "          [ 0.8801,  1.1203, -0.3608,  0.2028,  1.0607],\n",
      "          [ 1.9391, -1.7873,  0.5595, -1.8353,  0.1428]],\n",
      "\n",
      "         [[ 0.8198,  0.1454,  0.8159,  0.1229,  1.5083],\n",
      "          [ 0.9556,  0.2488, -0.1840,  1.4478, -1.3043],\n",
      "          [ 3.2645, -0.2925, -0.8358, -1.4526,  1.1776],\n",
      "          [ 0.7475,  0.4615, -1.4771, -1.0727,  1.7725],\n",
      "          [ 0.1590,  0.9290,  0.7634,  0.0162,  0.1811]],\n",
      "\n",
      "         [[ 0.0025,  1.2021, -0.5359,  1.1295, -0.0715],\n",
      "          [-0.7581, -0.2115, -1.5043, -1.1289, -2.0019],\n",
      "          [ 1.2882,  0.7592,  0.0984,  0.6170, -1.6142],\n",
      "          [-1.3035, -0.7252, -1.0193,  0.1318, -1.1698],\n",
      "          [ 1.6949,  1.5665, -0.0811, -0.8744,  0.8005]]]]) ###\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 5, 5)\n",
    "b = torch.randn(4, 3, 5, 5)\n",
    "c = torch.nn.functional.conv2d(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f63cf76cd30>> ###\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9f9e8c5be43c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# a dummy target, for example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make it the same shape as output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4fdaf1efadd1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Max pooling over a (2, 2) window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4b72deb72335>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(data, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### {} ###\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mweight_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# Interpolate numbers as strings because some can be one-elem tuples as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 5, 5)\n",
    "y = torch.randn(2, 3, 3, 3)\n",
    "torch.nn.functional.conv2d(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "# conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "# conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "\n",
    "# conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
    "# conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
    "# conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n",
    "\n",
    "# adaptive_avg_pool1d(input, output_size) -> Tensor\n",
    "\n",
    "# avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n",
    "# avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n",
    "# avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n",
    "\n",
    "# elu_(input, alpha=1.) -> Tensor\n",
    "# hardtanh_(input, min_val=-1., max_val=1., inplace=False):\n",
    "# leaky_relu_(input, negative_slope=0.01) -> Tensor\n",
    "# logsigmoid(input) -> Tensor\n",
    "# prelu(input, weight) -> Tensor\n",
    "# relu_(input) -> Tensor\n",
    "# rrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n",
    "# selu_(input) -> Tensor\n",
    "# softplus(input, beta=1, threshold=20) -> Tensor\n",
    "# softshrink(input, lambd=0.5) -> Tensor\n",
    "# threshold_(input, threshold, value) -> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 3, 5, 5).cuda()\n",
    "b = torch.randn(4, 3, 3, 3).cuda()\n",
    "c = torch.randn(4, 4)\n",
    "d = torch.randn(4, 4)\n",
    "result = torch.nn.functional.conv2d(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_list = [elem[0] for elem in inspect.getmembers(F) if inspect.isfunction(o[1])]\n",
    "\n",
    "match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "regex_filt_lst = [\"conv[0-9]d\", \"foo\"]\n",
    "print(match_any(\"conv3d_transpose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nvtx_patch\n",
    "\n",
    "# Functional list\n",
    "@nvtx_patch()\n",
    "def patched_conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op: 'conv1d', input: %s, weight: %s, stride: %s, padding: %s, dilation: %s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO\n",
    "    op = fn(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv2d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO    \n",
    "    op = conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv3d():\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv3d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO \n",
    "    op = conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose1d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose2d', input:%s, weight:%s, stride:%s, padding:%s, output_padding:%s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose2d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose3d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose3d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_linear(input, weight, bias=None):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op: 'linear', input:%s}\")\n",
    "###### TODO \n",
    "    op = linear(input, weight, bias)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "# (input, p=0.5, training=False, inplace=False)\n",
    "def patched_dropout():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input, inplace=False) \n",
    "def patched_relu():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input)\n",
    "def patched_relu_():\n",
    "    pas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def func(*args, **kwargs):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print 'function name \"%s\"' % inspect.getframeinfo(frame)[2]\n",
    "    for i in args:\n",
    "        print \"    %s = %s\" % (i, values[i])\n",
    "    return [(i, values[i]) for i in args]\n",
    "\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "\n",
    "global abc\n",
    "\n",
    "\n",
    "def my_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        frame = inspect.currentframe()\n",
    "        v = inspect.getargvalues(frame)\n",
    "        argspec = inspect.getfullargspec(func)\n",
    "        formal_arg_names = argspec.args\n",
    "        s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "        for idx, val in enumerate(v.locals[\"args\"]):\n",
    "            name = formal_arg_names[idx]\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                name += \"_shape\"\n",
    "                val = tuple(val.size())\n",
    "            s += \"'%s':%s,\" % (name, str(val))\n",
    "        num_def=len(argspec.defaults)\n",
    "        defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "        overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "        defaults.update(overrides)\n",
    "        s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "        print(\"Pushing NVTX range: %s\" % s)\n",
    "                \n",
    "        func(*args, **kwargs)\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "import torch \n",
    "\n",
    "@my_decorator\n",
    "def say_whee(foo, bar, baz=42, qux=123):\n",
    "    print(\"### In say_whee\")\n",
    "    print(\"foo = %s\" % str(foo))\n",
    "    print(\"bar = %s\" % str(bar))\n",
    "    print(\"baz = %s\" % str(baz))\n",
    "    print(\"qux = %s\" % str(ham))\n",
    "\n",
    "    print(\"Whee!\")\n",
    "\n",
    "say_whee(torch.randn(2,2), 1, qux=456)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['func'].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = \"{\"\n",
    "# Need to record arg name from signature\n",
    "for arg in abc.locals['args']:\n",
    "    if arg(isinstance(torch.Tensor)):\n",
    "        dims = tuple(arg.size())\n",
    "\n",
    "# torch.Tensor\n",
    "\"{name:%s, args:%s, kwargs:%s}\" % (func_name, str(args), str(kwargs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "help(F.conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    if not initialized:\n",
    "        # do stuff\n",
    "    initialized = True\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patched_hardtanh():\n",
    "#     pass\n",
    "\n",
    "# def patched_hardtanh_():\n",
    "#     pass\n",
    "\n",
    "# def patched_relu6():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu_():\n",
    "#     pass\n",
    "\n",
    "# def patched_unfold():\n",
    "#     pass\n",
    "\n",
    "# def patched_fold():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def pathched_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold_():\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# leaky_relu\n",
    "# leaky_relu_\n",
    "# prelu\n",
    "# rrelu\n",
    "# rrelu_\n",
    "# logsigmoid\n",
    "# hardshrink\n",
    "# tanhshrink\n",
    "# softplus\n",
    "# softmin\n",
    "# softmax\n",
    "# softshrink\n",
    "# gumbel_softmax\n",
    "# log_softmax\n",
    "# tanh\n",
    "# sigmoid\n",
    "# batch_norm\n",
    "# instance_norm\n",
    "# normalize\n",
    "\n",
    "# bilinear\n",
    "# alpha_dropout\n",
    "# dropout2d\n",
    "# dropout3d\n",
    "# pad\n",
    "\n",
    "\n",
    "# local_response_norm\n",
    "\n",
    "# embedding\n",
    "# embedding_bag\n",
    "# pairwise_distance\n",
    "# cosine_similarity\n",
    "# binary_cross_entropy\n",
    "# poisson_nll_loss\n",
    "# cosine_embedding_loss\n",
    "# cross_entropy\n",
    "# hinge_embedding_loss\n",
    "# kl_div\n",
    "# l1_loss\n",
    "# mse_loss\n",
    "# margin_ranking_loss\n",
    "# multilabel_margin_loss\n",
    "# multilabel_soft_margin_loss\n",
    "# multi_margin_loss\n",
    "# nll_loss\n",
    "# binary_cross_entropy_with_logits\n",
    "# smooth_l1_loss\n",
    "# soft_margin_loss\n",
    "# triplet_margin_loss\n",
    "# pixel_shuffle\n",
    "# grid_sample\n",
    "# affine_grid\n",
    "# data_parallel\n",
    "# calculate_gain\n",
    "# uniform_\n",
    "# normal_\n",
    "# constant_\n",
    "# eye_\n",
    "# dirac_\n",
    "# xavier_uniform_\n",
    "# xavier_normal\n",
    "# kaiming_uniform_\n",
    "# kaiming_normal_\n",
    "# orthogonal_\n",
    "# sparse_\n",
    "\n",
    "# upsample\n",
    "\n",
    "# upsample_nearest\n",
    "# upsample_bilinear\n",
    "# interpolate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
