{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.nvtx as nvtx\n",
    "import inspect\n",
    "from inspect import currentframe, getargvalues, getfullargspec, getmembers, isfunction\n",
    "import types\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions registered for NVTX range annotation:\n",
      "['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d', 'adaptive_max_pool1d', 'adaptive_max_pool2d', 'adaptive_max_pool3d', 'affine_grid', 'alpha_dropout', 'avg_pool1d', 'avg_pool2d', 'avg_pool3d', 'batch_norm', 'bilinear', 'binary_cross_entropy', 'binary_cross_entropy_with_logits', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'cosine_embedding_loss', 'cosine_similarity', 'cross_entropy', 'dropout', 'dropout2d', 'dropout3d', 'elu', 'elu_', 'embedding', 'embedding_bag', 'fold', 'glu', 'grid_sample', 'gumbel_softmax', 'hardshrink', 'hardtanh', 'hardtanh_', 'hinge_embedding_loss', 'instance_norm', 'interpolate', 'kl_div', 'l1_loss', 'layer_norm', 'leaky_relu', 'leaky_relu_', 'linear', 'local_response_norm', 'log_softmax', 'logsigmoid', 'lp_pool1d', 'lp_pool2d', 'max_pool1d', 'max_pool2d', 'max_pool3d', 'max_unpool1d', 'max_unpool2d', 'max_unpool3d', 'mse_loss', 'nll_loss', 'normalize', 'pad', 'pairwise_distance', 'pixel_shuffle', 'poisson_nll_loss', 'prelu', 'relu', 'relu6', 'relu_', 'rrelu', 'rrelu_', 'selu', 'selu_', 'sigmoid', 'smooth_l1_loss', 'soft_margin_loss', 'softmax', 'softmin', 'softplus', 'softshrink', 'softsign', 'tanh', 'tanhshrink', 'threshold', 'threshold_', 'triplet_margin_loss', 'unfold', 'upsample', 'upsample_bilinear', 'upsample_nearest']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NvtxPatcher:\n",
    "    \n",
    "    registry = set()\n",
    "    nvtx_handle = nvtx._libnvToolsExt()\n",
    "    \n",
    "    @staticmethod\n",
    "    def nvtx_monkey_patch(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            frame = currentframe()\n",
    "            v = getargvalues(frame)\n",
    "            print(\"FUNC = {}\".format(func))\n",
    "            argspec = getfullargspec(func)\n",
    "            formal_arg_names = argspec.args\n",
    "            s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "            for idx, val in enumerate(v.locals[\"args\"]):\n",
    "                name = formal_arg_names[idx]\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    name += \"_shape\"\n",
    "                    val = tuple(val.size())\n",
    "                s += \"'%s':%s,\" % (name, str(val))\n",
    "            num_def=len(argspec.defaults)\n",
    "            defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "            overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "            defaults.update(overrides)\n",
    "            s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "            nvtx.range_push(s)\n",
    "            result = func(*args, **kwargs)\n",
    "            nvtx.range_pop()\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    @classmethod\n",
    "    def register_module(cls, module, regex_filt_lst=None, log=True):\n",
    "        if not isinstance(regex_filt_lst, list) and regex_filt_lst is not None:\n",
    "            regex_filt_lst = list(regex_filt_lst)\n",
    "        if isinstance(module, str):\n",
    "            module = eval(module)\n",
    "        name_list = dir(module)\n",
    "        mod_funcs = [_a for _a in name_list if\n",
    "                     (isinstance(getattr(module, _a), types.FunctionType) or\n",
    "                      isinstance(getattr(module, _a), types.BuiltinFunctionType) or\n",
    "                     isinstance(getattr(module, _a), types.BuiltinMethodType))]\n",
    "        \n",
    "        match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "        if regex_filt_lst is not None:\n",
    "            function_list = [_x for _x in mod_funcs if match_any(_x)]\n",
    "        else: \n",
    "            function_list = [_x for _x in mod_funcs]\n",
    "            \n",
    "        for name in function_list:\n",
    "            if name in cls.registry:\n",
    "                continue\n",
    "            fqn = \"{}.{}\".format(module.__name__, name)\n",
    "            temp = eval(fqn)\n",
    "            patched = NvtxPatcher.nvtx_monkey_patch(temp)\n",
    "            cls.registry.add(fqn)\n",
    "            exec(\"{}=patched\".format(fqn))\n",
    "            \n",
    "        print(\"{}\\n{}\\n\".format(\"Functions registered for NVTX range annotation:\", function_list))\n",
    "        \n",
    "np = NvtxPatcher.register_module(torch.nn.functional,\n",
    "                                 [\"conv[1-3]?(d|(\\_transpose[1-3]d))\",\n",
    "                                 \"(un)?fold\",\n",
    "                                  \"(avg|max)_pool\",\n",
    "                                 \"max_unpool[1-3]d\",\n",
    "                                 \"lp_pool[1-3]d\",\n",
    "                                 \"adaptive_(avg|max)_pool[1-3]d\",\n",
    "                                 \"threshold\",\n",
    "                                 \"(leaky_)?[p-s]?r?elu_?6?\",\n",
    "                                 \"(hard)?tanh\",\n",
    "                                 \"glu\",\n",
    "                                 \"(log)?sigmoid\",\n",
    "                                 \"(hard|soft|tanh)shrink\",\n",
    "                                 \"soft(sign|plus|min)\",\n",
    "                                 \"(gumbel_|log_)?softmax\",\n",
    "                                 \"(batch|layer|instance|local_response)_norm\",\n",
    "                                 \"normalize\",\n",
    "                                 \"(bi)?linear\",\n",
    "                                 \"(alpha_)?dropout([2-3]d)?\",\n",
    "                                 \"embedding(_bag)?\",\n",
    "                                 \"pairwise_distance\",\n",
    "                                 \"cosine_similarity\",\n",
    "                                 \"(binary_)?cross_entropy\",\n",
    "                                 \"(poisson_)?nll_loss\",\n",
    "                                 \"(cosine|hinge)_embedding_loss\",\n",
    "                                 \"kl_div\",\n",
    "                                 \"((smooth_)?l1|mse)_loss\",\n",
    "                                 \"(multilabel|multi)?_margin_(soft_?)(ranking)?_loss\",\n",
    "                                 \"(soft|triplet)_margin_loss\",\n",
    "                                 \"pad\",\n",
    "                                 \"pixel_shuffle\",\n",
    "                                 \"interpolate\",\n",
    "                                 \"upsample_?(bilinear|nearest)?\",\n",
    "                                 \"(affine_)?grid(_sample)?\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNC = <built-in method conv2d of type object at 0x7fcf08bd8140>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                                        \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m                                        sigcls=Signature)\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2265\u001b[0m         return _signature_from_builtin(sigcls, obj,\n\u001b[0;32m-> 2266\u001b[0;31m                                        skip_bound_arg=skip_bound_arg)\n\u001b[0m\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_builtin\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2090\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no signature found for builtin {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: no signature found for builtin <built-in method conv2d of type object at 0x7fcf08bd8140>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5f96bd6ffeb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f8db288ede32>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetargvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FUNC = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mformal_arg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{'op':%s,\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# else. So to be fully backwards compatible, we catch all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[0;31m# possible exceptions here, and reraise a TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported callable'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported callable"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 5, 5).cuda()\n",
    "b = torch.randn(4, 3, 3, 3).cuda()\n",
    "c = torch.randn(4, 4)\n",
    "d = torch.randn(4, 4)\n",
    "result = torch.nn.functional.conv2d(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2689, -0.5198,  0.1525,  0.6487],\n",
      "        [-1.3071, -1.6274,  0.4481,  0.6504],\n",
      "        [ 0.9134, -0.3196, -1.7626,  2.1990],\n",
      "        [-1.6266, -0.2472,  1.7105, -0.0705]])\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_list = [elem[0] for elem in inspect.getmembers(F) if inspect.isfunction(o[1])]\n",
    "\n",
    "match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "regex_filt_lst = [\"conv[0-9]d\", \"foo\"]\n",
    "print(match_any(\"conv3d_transpose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nvtx_patch\n",
    "\n",
    "# Functional list\n",
    "@nvtx_patch()\n",
    "def patched_conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op: 'conv1d', input: %s, weight: %s, stride: %s, padding: %s, dilation: %s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO\n",
    "    op = fn(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv2d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO    \n",
    "    op = conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv3d():\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv3d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO \n",
    "    op = conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose1d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose2d', input:%s, weight:%s, stride:%s, padding:%s, output_padding:%s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose2d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose3d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose3d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_linear(input, weight, bias=None):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op: 'linear', input:%s}\")\n",
    "###### TODO \n",
    "    op = linear(input, weight, bias)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "# (input, p=0.5, training=False, inplace=False)\n",
    "def patched_dropout():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input, inplace=False) \n",
    "def patched_relu():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input)\n",
    "def patched_relu_():\n",
    "    pas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def func(*args, **kwargs):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print 'function name \"%s\"' % inspect.getframeinfo(frame)[2]\n",
    "    for i in args:\n",
    "        print \"    %s = %s\" % (i, values[i])\n",
    "    return [(i, values[i]) for i in args]\n",
    "\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "\n",
    "global abc\n",
    "\n",
    "\n",
    "def my_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        frame = inspect.currentframe()\n",
    "        v = inspect.getargvalues(frame)\n",
    "        argspec = inspect.getfullargspec(func)\n",
    "        formal_arg_names = argspec.args\n",
    "        s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "        for idx, val in enumerate(v.locals[\"args\"]):\n",
    "            name = formal_arg_names[idx]\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                name += \"_shape\"\n",
    "                val = tuple(val.size())\n",
    "            s += \"'%s':%s,\" % (name, str(val))\n",
    "        num_def=len(argspec.defaults)\n",
    "        defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "        overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "        defaults.update(overrides)\n",
    "        s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "        print(\"Pushing NVTX range: %s\" % s)\n",
    "                \n",
    "        func(*args, **kwargs)\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "import torch \n",
    "\n",
    "@my_decorator\n",
    "def say_whee(foo, bar, baz=42, qux=123):\n",
    "    print(\"### In say_whee\")\n",
    "    print(\"foo = %s\" % str(foo))\n",
    "    print(\"bar = %s\" % str(bar))\n",
    "    print(\"baz = %s\" % str(baz))\n",
    "    print(\"qux = %s\" % str(ham))\n",
    "\n",
    "    print(\"Whee!\")\n",
    "\n",
    "say_whee(torch.randn(2,2), 1, qux=456)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['func'].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = \"{\"\n",
    "# Need to record arg name from signature\n",
    "for arg in abc.locals['args']:\n",
    "    if arg(isinstance(torch.Tensor)):\n",
    "        dims = tuple(arg.size())\n",
    "\n",
    "# torch.Tensor\n",
    "\"{name:%s, args:%s, kwargs:%s}\" % (func_name, str(args), str(kwargs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "help(F.conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    if not initialized:\n",
    "        # do stuff\n",
    "    initialized = True\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patched_hardtanh():\n",
    "#     pass\n",
    "\n",
    "# def patched_hardtanh_():\n",
    "#     pass\n",
    "\n",
    "# def patched_relu6():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu_():\n",
    "#     pass\n",
    "\n",
    "# def patched_unfold():\n",
    "#     pass\n",
    "\n",
    "# def patched_fold():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def pathched_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold_():\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# leaky_relu\n",
    "# leaky_relu_\n",
    "# prelu\n",
    "# rrelu\n",
    "# rrelu_\n",
    "# logsigmoid\n",
    "# hardshrink\n",
    "# tanhshrink\n",
    "# softplus\n",
    "# softmin\n",
    "# softmax\n",
    "# softshrink\n",
    "# gumbel_softmax\n",
    "# log_softmax\n",
    "# tanh\n",
    "# sigmoid\n",
    "# batch_norm\n",
    "# instance_norm\n",
    "# normalize\n",
    "\n",
    "# bilinear\n",
    "# alpha_dropout\n",
    "# dropout2d\n",
    "# dropout3d\n",
    "# pad\n",
    "\n",
    "\n",
    "# local_response_norm\n",
    "\n",
    "# embedding\n",
    "# embedding_bag\n",
    "# pairwise_distance\n",
    "# cosine_similarity\n",
    "# binary_cross_entropy\n",
    "# poisson_nll_loss\n",
    "# cosine_embedding_loss\n",
    "# cross_entropy\n",
    "# hinge_embedding_loss\n",
    "# kl_div\n",
    "# l1_loss\n",
    "# mse_loss\n",
    "# margin_ranking_loss\n",
    "# multilabel_margin_loss\n",
    "# multilabel_soft_margin_loss\n",
    "# multi_margin_loss\n",
    "# nll_loss\n",
    "# binary_cross_entropy_with_logits\n",
    "# smooth_l1_loss\n",
    "# soft_margin_loss\n",
    "# triplet_margin_loss\n",
    "# pixel_shuffle\n",
    "# grid_sample\n",
    "# affine_grid\n",
    "# data_parallel\n",
    "# calculate_gain\n",
    "# uniform_\n",
    "# normal_\n",
    "# constant_\n",
    "# eye_\n",
    "# dirac_\n",
    "# xavier_uniform_\n",
    "# xavier_normal\n",
    "# kaiming_uniform_\n",
    "# kaiming_normal_\n",
    "# orthogonal_\n",
    "# sparse_\n",
    "\n",
    "# upsample\n",
    "\n",
    "# upsample_nearest\n",
    "# upsample_bilinear\n",
    "# interpolate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
