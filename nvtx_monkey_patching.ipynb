{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.nvtx as nvtx\n",
    "import inspect\n",
    "from inspect import currentframe, getargvalues, getfullargspec, getmembers, isfunction\n",
    "import types\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions registered for NVTX range annotation:\n",
      "['adaptive_avg_pool2d', 'adaptive_avg_pool3d', 'adaptive_max_pool1d', 'adaptive_max_pool2d', 'adaptive_max_pool3d', 'affine_grid', 'alpha_dropout', 'batch_norm', 'bilinear', 'binary_cross_entropy', 'binary_cross_entropy_with_logits', 'cosine_embedding_loss', 'cosine_similarity', 'cross_entropy', 'dropout', 'dropout2d', 'dropout3d', 'elu', 'embedding', 'embedding_bag', 'fold', 'glu', 'grid_sample', 'gumbel_softmax', 'hardshrink', 'hardtanh', 'hinge_embedding_loss', 'instance_norm', 'interpolate', 'kl_div', 'l1_loss', 'layer_norm', 'leaky_relu', 'linear', 'local_response_norm', 'log_softmax', 'lp_pool1d', 'lp_pool2d', 'max_pool1d', 'max_pool2d', 'max_pool3d', 'max_unpool1d', 'max_unpool2d', 'max_unpool3d', 'mse_loss', 'nll_loss', 'normalize', 'pad', 'pairwise_distance', 'pixel_shuffle', 'poisson_nll_loss', 'relu', 'relu6', 'rrelu', 'selu', 'sigmoid', 'smooth_l1_loss', 'soft_margin_loss', 'softmax', 'softmin', 'softsign', 'tanh', 'tanhshrink', 'threshold', 'triplet_margin_loss', 'unfold', 'upsample', 'upsample_bilinear', 'upsample_nearest']\n",
      "\n",
      "built-ins (manual monkey-patching required):\n",
      "['adaptive_avg_pool1d', 'avg_pool1d', 'avg_pool2d', 'avg_pool3d', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'elu_', 'hardtanh_', 'leaky_relu_', 'logsigmoid', 'prelu', 'relu_', 'rrelu_', 'selu_', 'softplus', 'softshrink', 'threshold_']\n"
     ]
    }
   ],
   "source": [
    "class NvtxPatcher:\n",
    "    \n",
    "    registry = set()\n",
    "    nvtx_handle = nvtx._libnvToolsExt()\n",
    "    \n",
    "    @staticmethod\n",
    "    def nvtx_monkey_patch(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            frame = currentframe()\n",
    "            v = getargvalues(frame)\n",
    "            print(\"FUNC = {}\".format(func))\n",
    "            argspec = getfullargspec(func)\n",
    "            formal_arg_names = argspec.args\n",
    "            s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "            for idx, val in enumerate(v.locals[\"args\"]):\n",
    "                name = formal_arg_names[idx]\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    name += \"_shape\"\n",
    "                    val = tuple(val.size())\n",
    "                s += \"'%s':%s,\" % (name, str(val))\n",
    "            num_def=len(argspec.defaults)\n",
    "            defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "            overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "            defaults.update(overrides)\n",
    "            s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "            nvtx.range_push(s)\n",
    "            result = func(*args, **kwargs)\n",
    "            nvtx.range_pop()\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    @classmethod\n",
    "    def list_non_builtins(cls, module, regex_filt_lst=None, log=True):\n",
    "        if not isinstance(regex_filt_lst, list) and regex_filt_lst is not None:\n",
    "            regex_filt_lst = list(regex_filt_lst)\n",
    "        if isinstance(module, str):\n",
    "            module = eval(module)\n",
    "        name_list = dir(module)\n",
    "        builtin_funcs_methods = [_a for _a in name_list if\n",
    "                                 (isinstance(getattr(module, _a), types.BuiltinFunctionType) or\n",
    "                                  isinstance(getattr(module, _a), types.BuiltinMethodType))]\n",
    "        match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "        if regex_filt_lst is not None:\n",
    "            function_list = [_x for _x in builtin_funcs_methods if match_any(_x)]\n",
    "        else: \n",
    "            function_list = [_x for _x in builtin_funcs_methods]\n",
    "        return function_list \n",
    "                                 \n",
    "    @classmethod\n",
    "    def register_non_builtins(cls, module, regex_filt_lst=None, log=True):\n",
    "        if not isinstance(regex_filt_lst, list) and regex_filt_lst is not None:\n",
    "            regex_filt_lst = list(regex_filt_lst)\n",
    "        if isinstance(module, str):\n",
    "            module = eval(module)\n",
    "        name_list = dir(module)\n",
    "        non_builtin_funcs = [_a for _a in name_list if\n",
    "                     isinstance(getattr(module, _a), types.FunctionType)]\n",
    "        \n",
    "        match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "        if regex_filt_lst is not None:\n",
    "            function_list = [_x for _x in non_builtin_funcs if match_any(_x)]\n",
    "        else: \n",
    "            function_list = [_x for _x in mod_funcs]\n",
    "            \n",
    "        for name in function_list:\n",
    "            if name in cls.registry:\n",
    "                continue\n",
    "            fqn = \"{}.{}\".format(module.__name__, name)\n",
    "            temp = eval(fqn)\n",
    "            patched = NvtxPatcher.nvtx_monkey_patch(temp)\n",
    "            cls.registry.add(fqn)\n",
    "            exec(\"{}=patched\".format(fqn))\n",
    "            \n",
    "        print(\"{}\\n{}\\n\".format(\"Functions registered for NVTX range annotation:\", function_list))\n",
    "    \n",
    "patterns = [\"conv[1-3]?(d|(\\_transpose[1-3]d))\",\n",
    "     \"(un)?fold\",\n",
    "     \"(avg|max)_pool\",\n",
    "     \"max_unpool[1-3]d\",\n",
    "     \"lp_pool[1-3]d\",\n",
    "     \"adaptive_(avg|max)_pool[1-3]d\",\n",
    "     \"threshold\",\n",
    "     \"(leaky_)?[p-s]?r?elu_?6?\",\n",
    "     \"(hard)?tanh\",\n",
    "     \"glu\",\n",
    "     \"(log)?sigmoid\",\n",
    "     \"(hard|soft|tanh)shrink\",\n",
    "     \"soft(sign|plus|min)\",\n",
    "     \"(gumbel_|log_)?softmax\",\n",
    "     \"(batch|layer|instance|local_response)_norm\",\n",
    "     \"normalize\",\n",
    "     \"(bi)?linear\",\n",
    "     \"(alpha_)?dropout([2-3]d)?\",\n",
    "     \"embedding(_bag)?\",\n",
    "     \"pairwise_distance\",\n",
    "     \"cosine_similarity\",\n",
    "     \"(binary_)?cross_entropy\",\n",
    "     \"(poisson_)?nll_loss\",\n",
    "     \"(cosine|hinge)_embedding_loss\",\n",
    "     \"kl_div\",\n",
    "     \"((smooth_)?l1|mse)_loss\",\n",
    "     \"(multilabel|multi)?_margin_(soft_?)(ranking)?_loss\",\n",
    "     \"(soft|triplet)_margin_loss\",\n",
    "     \"pad\",\n",
    "     \"pixel_shuffle\",\n",
    "     \"interpolate\",\n",
    "     \"upsample_?(bilinear|nearest)?\",\n",
    "     \"(affine_)?grid(_sample)?\"]\n",
    "\n",
    "NvtxPatcher.register_non_builtins(\n",
    "    torch.nn.functional, patterns)\n",
    "\n",
    "print(\"built-ins (manual monkey-patching required):\")\n",
    "print(NvtxPatcher.list_non_builtins(torch.nn.functional, patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the following:\n",
    "adaptive_avg_pool1d\n",
    "\n",
    "avg_pool1d\n",
    "avg_pool2d\n",
    "avg_pool3d\n",
    "\n",
    "conv1d \n",
    "conv2d\n",
    "conv3d\n",
    "\n",
    "conv_transpose1d\n",
    "conv_transpose2d\n",
    "conv_transpose3d\n",
    "\n",
    "elu_\n",
    "hardtanh_\n",
    "leaky_relu_\n",
    "logsigmoid\n",
    "prelu\n",
    "relu_\n",
    "rrelu_\n",
    "selu_\n",
    "softplus\n",
    "softshrink\n",
    "threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 3, 5, 5).cuda()\n",
    "b = torch.randn(4, 3, 3, 3).cuda()\n",
    "c = torch.randn(4, 4)\n",
    "d = torch.randn(4, 4)\n",
    "result = torch.nn.functional.conv2d(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_list = [elem[0] for elem in inspect.getmembers(F) if inspect.isfunction(o[1])]\n",
    "\n",
    "match_any = lambda txt:  any((map(lambda x: re.match(r\"%s\" % x, txt), regex_filt_lst)))\n",
    "regex_filt_lst = [\"conv[0-9]d\", \"foo\"]\n",
    "print(match_any(\"conv3d_transpose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nvtx_patch\n",
    "\n",
    "# Functional list\n",
    "@nvtx_patch()\n",
    "def patched_conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op: 'conv1d', input: %s, weight: %s, stride: %s, padding: %s, dilation: %s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO\n",
    "    op = fn(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv2d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO    \n",
    "    op = conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv3d():\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    # Interpolate numbers as strings because some can be one-elem tuples as well\n",
    "    nvtx.range_push(\"{op:'conv3d', input:%s, weight:%s, stride:%s, padding:%s, dilation:%s, groups:%s}\" % \n",
    "                    (input_size, weight_size, str(stride), str(padding), str(dilation), str(groups)))\n",
    "###### TODO \n",
    "    op = conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose1d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose2d', input:%s, weight:%s, stride:%s, padding:%s, output_padding:%s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose2d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op:'conv_transpose3d', input:%s, weight:%s, stride:%s, padding:%s, output_padding: %s, groups:%s, dilation:%s}\"\n",
    "                    % (input_size, weight_size, str(stride), str(padding), str(output_padding), str(groups), str(dilation)))\n",
    "###### TODO \n",
    "    op = conv_transpose3d(input, weight, bias, stride, padding, output_padding, groups, dilation)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "def patched_linear(input, weight, bias=None):\n",
    "    input_size = tuple(input.size())\n",
    "    weight_size = tuple(weight.size())\n",
    "    nvtx.range_push(\"{op: 'linear', input:%s}\")\n",
    "###### TODO \n",
    "    op = linear(input, weight, bias)\n",
    "    nvtx.range_pop()\n",
    "    return op\n",
    "\n",
    "@nvtx_patch()\n",
    "# (input, p=0.5, training=False, inplace=False)\n",
    "def patched_dropout():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input, inplace=False) \n",
    "def patched_relu():\n",
    "    pass\n",
    "\n",
    "@nvtx_patch()    \n",
    "# (input)\n",
    "def patched_relu_():\n",
    "    pas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def func(*args, **kwargs):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print 'function name \"%s\"' % inspect.getframeinfo(frame)[2]\n",
    "    for i in args:\n",
    "        print \"    %s = %s\" % (i, values[i])\n",
    "    return [(i, values[i]) for i in args]\n",
    "\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "\n",
    "global abc\n",
    "\n",
    "\n",
    "def my_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        frame = inspect.currentframe()\n",
    "        v = inspect.getargvalues(frame)\n",
    "        argspec = inspect.getfullargspec(func)\n",
    "        formal_arg_names = argspec.args\n",
    "        s = \"{'op':%s,\" % v.locals[\"func\"].__name__\n",
    "        for idx, val in enumerate(v.locals[\"args\"]):\n",
    "            name = formal_arg_names[idx]\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                name += \"_shape\"\n",
    "                val = tuple(val.size())\n",
    "            s += \"'%s':%s,\" % (name, str(val))\n",
    "        num_def=len(argspec.defaults)\n",
    "        defaults = dict(zip(argspec.args[-num_def:], argspec.defaults))\n",
    "        overrides={k:str(v) for k, v in v.locals[\"kwargs\"].items()}\n",
    "        defaults.update(overrides)\n",
    "        s += \"%s}\" % str(defaults).strip(\"{}\")\n",
    "        print(\"Pushing NVTX range: %s\" % s)\n",
    "                \n",
    "        func(*args, **kwargs)\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "import torch \n",
    "\n",
    "@my_decorator\n",
    "def say_whee(foo, bar, baz=42, qux=123):\n",
    "    print(\"### In say_whee\")\n",
    "    print(\"foo = %s\" % str(foo))\n",
    "    print(\"bar = %s\" % str(bar))\n",
    "    print(\"baz = %s\" % str(baz))\n",
    "    print(\"qux = %s\" % str(ham))\n",
    "\n",
    "    print(\"Whee!\")\n",
    "\n",
    "say_whee(torch.randn(2,2), 1, qux=456)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['kwargs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.locals['func'].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = \"{\"\n",
    "# Need to record arg name from signature\n",
    "for arg in abc.locals['args']:\n",
    "    if arg(isinstance(torch.Tensor)):\n",
    "        dims = tuple(arg.size())\n",
    "\n",
    "# torch.Tensor\n",
    "\"{name:%s, args:%s, kwargs:%s}\" % (func_name, str(args), str(kwargs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "help(F.conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    if not initialized:\n",
    "        # do stuff\n",
    "    initialized = True\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patched_hardtanh():\n",
    "#     pass\n",
    "\n",
    "# def patched_hardtanh_():\n",
    "#     pass\n",
    "\n",
    "# def patched_relu6():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu():\n",
    "#     pass\n",
    "\n",
    "# def patched_elu_():\n",
    "#     pass\n",
    "\n",
    "# def patched_unfold():\n",
    "#     pass\n",
    "\n",
    "# def patched_fold():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def pathched_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_max_unpool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_lp_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_max_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool1d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool2d():\n",
    "#     pass\n",
    "\n",
    "# def patched_adaptive_avg_pool3d():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold():\n",
    "#     pass\n",
    "\n",
    "# def patched_threshold_():\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# leaky_relu\n",
    "# leaky_relu_\n",
    "# prelu\n",
    "# rrelu\n",
    "# rrelu_\n",
    "# logsigmoid\n",
    "# hardshrink\n",
    "# tanhshrink\n",
    "# softplus\n",
    "# softmin\n",
    "# softmax\n",
    "# softshrink\n",
    "# gumbel_softmax\n",
    "# log_softmax\n",
    "# tanh\n",
    "# sigmoid\n",
    "# batch_norm\n",
    "# instance_norm\n",
    "# normalize\n",
    "\n",
    "# bilinear\n",
    "# alpha_dropout\n",
    "# dropout2d\n",
    "# dropout3d\n",
    "# pad\n",
    "\n",
    "\n",
    "# local_response_norm\n",
    "\n",
    "# embedding\n",
    "# embedding_bag\n",
    "# pairwise_distance\n",
    "# cosine_similarity\n",
    "# binary_cross_entropy\n",
    "# poisson_nll_loss\n",
    "# cosine_embedding_loss\n",
    "# cross_entropy\n",
    "# hinge_embedding_loss\n",
    "# kl_div\n",
    "# l1_loss\n",
    "# mse_loss\n",
    "# margin_ranking_loss\n",
    "# multilabel_margin_loss\n",
    "# multilabel_soft_margin_loss\n",
    "# multi_margin_loss\n",
    "# nll_loss\n",
    "# binary_cross_entropy_with_logits\n",
    "# smooth_l1_loss\n",
    "# soft_margin_loss\n",
    "# triplet_margin_loss\n",
    "# pixel_shuffle\n",
    "# grid_sample\n",
    "# affine_grid\n",
    "# data_parallel\n",
    "# calculate_gain\n",
    "# uniform_\n",
    "# normal_\n",
    "# constant_\n",
    "# eye_\n",
    "# dirac_\n",
    "# xavier_uniform_\n",
    "# xavier_normal\n",
    "# kaiming_uniform_\n",
    "# kaiming_normal_\n",
    "# orthogonal_\n",
    "# sparse_\n",
    "\n",
    "# upsample\n",
    "\n",
    "# upsample_nearest\n",
    "# upsample_bilinear\n",
    "# interpolate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
